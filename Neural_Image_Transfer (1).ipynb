{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Image_Transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "E6YSl92S95xa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "from six.moves import urllib\n",
        "\n",
        "def download(download_link, file_name, expected_bytes):\n",
        "    \"\"\" Download the pretrained VGG-19 model if it's not already downloaded \"\"\"\n",
        "    if os.path.exists(file_name):\n",
        "        print(\"VGG-19 pre-trained model is ready\")\n",
        "        return\n",
        "    print(\"Downloading the VGG pre-trained model. This might take a while ...\")\n",
        "    file_name, _ = urllib.request.urlretrieve(download_link, file_name)\n",
        "    file_stat = os.stat(file_name)\n",
        "    if file_stat.st_size == expected_bytes:\n",
        "        print('Successfully downloaded VGG-19 pre-trained model', file_name)\n",
        "    else:\n",
        "        raise Exception('File ' + file_name +\n",
        "                        ' might be corrupted. You should try downloading it with a browser.')\n",
        "\n",
        "def get_resized_image(img_path, width, height, save=True):\n",
        "    image = Image.open(img_path)\n",
        "    # PIL is column major so you have to swap the places of width and height\n",
        "    image = ImageOps.fit(image, (width, height), Image.ANTIALIAS)\n",
        "    if save:\n",
        "        image_dirs = img_path.split('/')\n",
        "        image_dirs[-1] = 'resized_' + image_dirs[-1]\n",
        "        out_path = '/'.join(image_dirs)\n",
        "        if not os.path.exists(out_path):\n",
        "            image.save(out_path)\n",
        "    image = np.asarray(image, np.float32)\n",
        "    return np.expand_dims(image, 0)\n",
        "\n",
        "def generate_noise_image(content_image, width, height, noise_ratio=0.6):\n",
        "    noise_image = np.random.uniform(-20, 20, (1, height, width, 3)).astype(np.float32)\n",
        "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
        "\n",
        "def save_image(path, image):\n",
        "    image = image[0]\n",
        "    image = np.clip(image, 0, 255).astype('uint8')\n",
        "    scipy.misc.imsave(path, image)\n",
        "\n",
        "def safe_mkdir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GrTZiQH_8f1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "# VGG-19 parameters file\n",
        "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
        "VGG_FILENAME = 'imagenet-vgg-verydeep-19.mat'\n",
        "EXPECTED_BYTES = 534904783\n",
        "\n",
        "class VGG(object):\n",
        "    def __init__(self, input_img):\n",
        "        download(VGG_DOWNLOAD_LINK, VGG_FILENAME, EXPECTED_BYTES)\n",
        "        self.vgg_layers = scipy.io.loadmat(VGG_FILENAME)['layers']\n",
        "        self.input_img = input_img\n",
        "        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
        "\n",
        "    def _weights(self, layer_idx, expected_layer_name):\n",
        "        \"\"\" Return the weights and biases at layer_idx already trained by VGG\n",
        "        \"\"\"\n",
        "        W = self.vgg_layers[0][layer_idx][0][0][2][0][0]\n",
        "        b = self.vgg_layers[0][layer_idx][0][0][2][0][1]\n",
        "        layer_name = self.vgg_layers[0][layer_idx][0][0][0][0]\n",
        "        assert layer_name == expected_layer_name\n",
        "        return W, b.reshape(b.size)\n",
        "\n",
        "    def conv2d_relu(self, prev_layer, layer_idx, layer_name):\n",
        "        \"\"\" Return the Conv2D layer with RELU using the weights, \n",
        "        biases from the VGG model at 'layer_idx'.\n",
        "        Don't forget to apply relu to the output from the convolution.\n",
        "        Inputs:\n",
        "            prev_layer: the output tensor from the previous layer\n",
        "            layer_idx: the index to current layer in vgg_layers\n",
        "            layer_name: the string that is the name of the current layer.\n",
        "                        It's used to specify variable_scope.\n",
        "        Note that you first need to obtain W and b from from the corresponding VGG's layer \n",
        "        using the function _weights() defined above.\n",
        "        W and b returned from _weights() are numpy arrays, so you have\n",
        "        to convert them to TF tensors. One way to do it is with tf.constant.\n",
        "        Hint for choosing strides size: \n",
        "            for small images, you probably don't want to skip any pixel\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        ##\n",
        "        with tf.variable_scope(layer_name) as scope:\n",
        "            W, b = self._weights(layer_idx, layer_name)\n",
        "            W = tf.constant(W, name='weights')\n",
        "            b = tf.constant(b, name='bias')\n",
        "            conv2d = tf.nn.conv2d(prev_layer, \n",
        "                                filter=W, \n",
        "                                strides=[1, 1, 1, 1], \n",
        "                                padding='SAME')\n",
        "            out = tf.nn.relu(conv2d + b)\n",
        "        ###############################\n",
        "        setattr(self, layer_name, out)\n",
        "\n",
        "    def avgpool(self, prev_layer, layer_name):\n",
        "        \"\"\" Return the average pooling layer. The paper suggests that \n",
        "        average pooling works better than max pooling.\n",
        "        Input:\n",
        "            prev_layer: the output tensor from the previous layer\n",
        "            layer_name: the string that you want to name the layer.\n",
        "                        It's used to specify variable_scope.\n",
        "        Hint for choosing strides and kszie: choose what you feel appropriate\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "       \n",
        "        with tf.variable_scope(layer_name):\n",
        "            out = tf.nn.avg_pool(prev_layer, \n",
        "                                ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "        ###############################\n",
        "        setattr(self, layer_name, out)\n",
        "\n",
        "    def load(self):\n",
        "        self.conv2d_relu(self.input_img, 0, 'conv1_1')\n",
        "        self.conv2d_relu(self.conv1_1, 2, 'conv1_2')\n",
        "        self.avgpool(self.conv1_2, 'avgpool1')\n",
        "        self.conv2d_relu(self.avgpool1, 5, 'conv2_1')\n",
        "        self.conv2d_relu(self.conv2_1, 7, 'conv2_2')\n",
        "        self.avgpool(self.conv2_2, 'avgpool2')\n",
        "        self.conv2d_relu(self.avgpool2, 10, 'conv3_1')\n",
        "        self.conv2d_relu(self.conv3_1, 12, 'conv3_2')\n",
        "        self.conv2d_relu(self.conv3_2, 14, 'conv3_3')\n",
        "        self.conv2d_relu(self.conv3_3, 16, 'conv3_4')\n",
        "        self.avgpool(self.conv3_4, 'avgpool3')\n",
        "        self.conv2d_relu(self.avgpool3, 19, 'conv4_1')\n",
        "        self.conv2d_relu(self.conv4_1, 21, 'conv4_2')\n",
        "        self.conv2d_relu(self.conv4_2, 23, 'conv4_3')\n",
        "        self.conv2d_relu(self.conv4_3, 25, 'conv4_4')\n",
        "        self.avgpool(self.conv4_4, 'avgpool4')\n",
        "        self.conv2d_relu(self.avgpool4, 28, 'conv5_1')\n",
        "        self.conv2d_relu(self.conv5_1, 30, 'conv5_2')\n",
        "        self.conv2d_relu(self.conv5_2, 32, 'conv5_3')\n",
        "        self.conv2d_relu(self.conv5_3, 34, 'conv5_4')\n",
        "        self.avgpool(self.conv5_4, 'avgpool5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BrszyfKS-4vK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liJRXJA-7liA",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "a43c3990-da7f-4d73-f7b0-72c88ea40333"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4141063e-3958-43d2-9e06-113ddaa813d1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4141063e-3958-43d2-9e06-113ddaa813d1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving guernica.jpg to guernica (1).jpg\n",
            "Saving deadpool.jpg to deadpool (1).jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R2F2JydD7GD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        },
        "cellView": "both",
        "outputId": "4f296d22-17ce-4189-818a-f316526a8519"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup():\n",
        "    safe_mkdir('checkpoints')\n",
        "    safe_mkdir('outputs')\n",
        "\n",
        "class StyleTransfer(object):\n",
        "    def __init__(self, content_img, style_img, img_width, img_height):\n",
        "        '''\n",
        "        img_width and img_height are the dimensions we expect from the generated image.\n",
        "        We will resize input content image and input style image to match this dimension.\n",
        "        Feel free to alter any hyperparameter here and see how it affects your training.\n",
        "        '''\n",
        "        self.img_width = img_width\n",
        "        self.img_height = img_height\n",
        "        self.content_img = get_resized_image(content_img, img_width, img_height)\n",
        "        self.style_img = get_resized_image(style_img, img_width, img_height)\n",
        "        self.initial_img = generate_noise_image(self.content_img, img_width, img_height)\n",
        "\n",
        "        ###############################\n",
        "        ## \n",
        "        ## create global step (gstep) and hyperparameters for the model\n",
        "        self.content_layer = 'conv4_2'\n",
        "        self.style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
        "        self.content_w = 0.01\n",
        "        self.style_w = 1\n",
        "        self.style_layer_w = [0.5, 1.0, 1.5, 3.0, 4.0] \n",
        "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
        "                                trainable=False, name='global_step')\n",
        "        self.lr = 2.0\n",
        "        ###############################\n",
        "\n",
        "    def create_input(self):\n",
        "        '''\n",
        "        We will use one input_img as a placeholder for the content image, \n",
        "        style image, and generated image, because:\n",
        "            1. they have the same dimension\n",
        "            2. we have to extract the same set of features from them\n",
        "        We use a variable instead of a placeholder because we're, at the same time, \n",
        "        training the generated image to get the desirable result.\n",
        "        Note: image height corresponds to number of rows, not columns.\n",
        "        '''\n",
        "        with tf.variable_scope('input') as scope:\n",
        "            self.input_img = tf.get_variable('in_img', \n",
        "                                        shape=([1, self.img_height, self.img_width, 3]),\n",
        "                                        dtype=tf.float32,\n",
        "                                        initializer=tf.zeros_initializer())\n",
        "    def load_vgg(self):\n",
        "        '''\n",
        "        Load the saved model parameters of VGG-19, using the input_img\n",
        "        as the input to compute the output at each layer of vgg.\n",
        "        During training, VGG-19 mean-centered all images and found the mean pixels\n",
        "        to be [123.68, 116.779, 103.939] along RGB dimensions. We have to subtract\n",
        "        this mean from our images.\n",
        "        '''\n",
        "        self.vgg = VGG(self.input_img)\n",
        "        self.vgg.load()\n",
        "        self.content_img -= self.vgg.mean_pixels\n",
        "        self.style_img -= self.vgg.mean_pixels\n",
        "\n",
        "    def _content_loss(self, P, F):\n",
        "        ''' Calculate the loss between the feature representation of the\n",
        "        content image and the generated image.\n",
        "        \n",
        "        Inputs: \n",
        "            P: content representation of the content image\n",
        "            F: content representation of the generated image\n",
        "            Read the assignment handout for more details\n",
        "            Note: Don't use the coefficient 0.5 as defined in the paper.\n",
        "            Use the coefficient defined in the assignment handout.\n",
        "        '''\n",
        "        # self.content_loss = None\n",
        "        ###############################\n",
        "        ## \n",
        "        self.content_loss = tf.reduce_sum((F - P) ** 2) / (4.0 * P.size)\n",
        "        ###############################\n",
        "    \n",
        "    def _gram_matrix(self, F, N, M):\n",
        "        \"\"\" Create and return the gram matrix for tensor F\n",
        "            Hint: you'll first have to reshape F\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        ##\n",
        "        F = tf.reshape(F, (M, N))\n",
        "        return tf.matmul(tf.transpose(F), F)\n",
        "        ###############################\n",
        "\n",
        "    def _single_style_loss(self, a, g):\n",
        "        \"\"\" Calculate the style loss at a certain layer\n",
        "        Inputs:\n",
        "            a is the feature representation of the style image at that layer\n",
        "            g is the feature representation of the generated image at that layer\n",
        "        Output:\n",
        "            the style loss at a certain layer (which is E_l in the paper)\n",
        "        Hint: 1. you'll have to use the function _gram_matrix()\n",
        "            2. we'll use the same coefficient for style loss as in the paper\n",
        "            3. a and g are feature representation, not gram matrices\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        ##\n",
        "        N = a.shape[3] # number of filters\n",
        "        M = a.shape[1] * a.shape[2] # height times width of the feature map\n",
        "        A = self._gram_matrix(a, N, M)\n",
        "        G = self._gram_matrix(g, N, M)\n",
        "        return tf.reduce_sum((G - A) ** 2 / ((2 * N * M) ** 2))\n",
        "        ###############################\n",
        "\n",
        "    def _style_loss(self, A):\n",
        "        \"\"\" Calculate the total style loss as a weighted sum \n",
        "        of style losses at all style layers\n",
        "        Hint: you'll have to use _single_style_loss()\n",
        "        \"\"\"\n",
        "        n_layers = len(A)\n",
        "        E = [self._single_style_loss(A[i], getattr(self.vgg, self.style_layers[i])) for i in range(n_layers)]\n",
        "        \n",
        "        ###############################\n",
        "        ## \n",
        "        self.style_loss = sum([self.style_layer_w[i] * E[i] for i in range(n_layers)])\n",
        "        ###############################\n",
        "\n",
        "    def losses(self):\n",
        "        with tf.variable_scope('losses') as scope:\n",
        "            with tf.Session() as sess:\n",
        "                # assign content image to the input variable\n",
        "                sess.run(self.input_img.assign(self.content_img)) \n",
        "                gen_img_content = getattr(self.vgg, self.content_layer)\n",
        "                content_img_content = sess.run(gen_img_content)\n",
        "            self._content_loss(content_img_content, gen_img_content)\n",
        "\n",
        "            with tf.Session() as sess:\n",
        "                sess.run(self.input_img.assign(self.style_img))\n",
        "                style_layers = sess.run([getattr(self.vgg, layer) for layer in self.style_layers])                              \n",
        "            self._style_loss(style_layers)\n",
        "\n",
        "            ##########################################\n",
        "            ## create total loss. \n",
        "            ## don't forget the weights for the content loss and style loss\n",
        "            self.total_loss = self.content_w * self.content_loss + self.style_w * self.style_loss\n",
        "            ##########################################\n",
        "\n",
        "    def optimize(self):\n",
        "        ###############################\n",
        "        ## create optimizer\n",
        "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.total_loss,\n",
        "                                                            global_step=self.gstep)\n",
        "        ###############################\n",
        "\n",
        "    def create_summary(self):\n",
        "        ###############################\n",
        "        ## create summaries for all the losses\n",
        "        ##don't forget to merge them\n",
        "        with tf.name_scope('summaries'):\n",
        "            tf.summary.scalar('content loss', self.content_loss)\n",
        "            tf.summary.scalar('style loss', self.style_loss)\n",
        "            tf.summary.scalar('total loss', self.total_loss)\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "        ###############################\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        self.create_input()\n",
        "        self.load_vgg()\n",
        "        self.losses()\n",
        "        self.optimize()\n",
        "        self.create_summary()\n",
        "\n",
        "    def train(self, n_iters):\n",
        "        skip_step = 1\n",
        "        with tf.Session() as sess:\n",
        "            \n",
        "            ###############################\n",
        "            ##  \n",
        "            ##  initialize your variables\n",
        "            ##  create writer to write your graph\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            writer = tf.summary.FileWriter('graphs/style_stranfer', sess.graph)\n",
        "            ###############################\n",
        "            sess.run(self.input_img.assign(self.initial_img))\n",
        "\n",
        "\n",
        "            ###############################\n",
        "            ## \n",
        "            ##  create a saver object\n",
        "            ##  check if a checkpoint exists, restore the variables\n",
        "            saver = tf.train.Saver()\n",
        "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/style_transfer/checkpoint'))\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            ##############################\n",
        "\n",
        "            initial_step = self.gstep.eval()\n",
        "            \n",
        "            start_time = time.time()\n",
        "            for index in range(initial_step, n_iters):\n",
        "                if index >= 5 and index < 20:\n",
        "                    skip_step = 10\n",
        "                elif index >= 20:\n",
        "                    skip_step = 20\n",
        "                \n",
        "                sess.run(self.opt)\n",
        "                if (index + 1) % skip_step == 0:\n",
        "                    ###############################\n",
        "                    ## obtain generated image, loss, and summary\n",
        "                    gen_image, total_loss, summary = sess.run([self.input_img,\n",
        "                                                                self.total_loss,\n",
        "                                                                self.summary_op])\n",
        "\n",
        "                    ###############################\n",
        "                    \n",
        "                    # add back the mean pixels we subtracted before\n",
        "                    gen_image = gen_image + self.vgg.mean_pixels \n",
        "                    writer.add_summary(summary, global_step=index)\n",
        "                    print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
        "                    print('   Loss: {:5.1f}'.format(total_loss))\n",
        "                    print('   Took: {} seconds'.format(time.time() - start_time))\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    filename = 'outputs/%d.png' % (index)\n",
        "                    save_image(filename, gen_image)\n",
        "             \n",
        "                    if (index + 1) % 20 == 0:\n",
        "                        ###############################\n",
        "                        ## save the variables into a checkpoint\n",
        "                        saver.save(sess, 'checkpoints/style_stranfer/style_transfer', index)\n",
        "                        ###############################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup()\n",
        "    machine = StyleTransfer('deadpool.jpg', 'guernica.jpg', 333, 250)\n",
        "    machine.build()\n",
        "    machine.train(300)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG-19 pre-trained model is ready\n",
            "INFO:tensorflow:Summary name content loss is illegal; using content_loss instead.\n",
            "INFO:tensorflow:Summary name style loss is illegal; using style_loss instead.\n",
            "INFO:tensorflow:Summary name total loss is illegal; using total_loss instead.\n",
            "Step 1\n",
            "   Sum: 36145995.0\n",
            "   Loss: 2531523328.0\n",
            "   Took: 8.498481750488281 seconds\n",
            "Step 2\n",
            "   Sum: 36174099.7\n",
            "   Loss: 2274917120.0\n",
            "   Took: 4.3245689868927 seconds\n",
            "Step 3\n",
            "   Sum: 36203438.4\n",
            "   Loss: 2082149248.0\n",
            "   Took: 4.312552213668823 seconds\n",
            "Step 4\n",
            "   Sum: 36233623.1\n",
            "   Loss: 1927189248.0\n",
            "   Took: 4.285940170288086 seconds\n",
            "Step 5\n",
            "   Sum: 36263787.8\n",
            "   Loss: 1794451968.0\n",
            "   Took: 4.318542003631592 seconds\n",
            "Step 10\n",
            "   Sum: 36396115.2\n",
            "   Loss: 1299189120.0\n",
            "   Took: 16.157703161239624 seconds\n",
            "Step 20\n",
            "   Sum: 36516761.6\n",
            "   Loss: 831597056.0\n",
            "   Took: 30.67017149925232 seconds\n",
            "Step 40\n",
            "   Sum: 36336402.1\n",
            "   Loss: 466743328.0\n",
            "   Took: 59.728585720062256 seconds\n",
            "Step 60\n",
            "   Sum: 35966833.8\n",
            "   Loss: 319594368.0\n",
            "   Took: 60.34615468978882 seconds\n",
            "Step 80\n",
            "   Sum: 35562026.2\n",
            "   Loss: 241497488.0\n",
            "   Took: 60.44039535522461 seconds\n",
            "Step 100\n",
            "   Sum: 35160177.1\n",
            "   Loss: 193573968.0\n",
            "   Took: 61.50686836242676 seconds\n",
            "Step 120\n",
            "   Sum: 34766137.0\n",
            "   Loss: 160037408.0\n",
            "   Took: 59.21093273162842 seconds\n",
            "Step 140\n",
            "   Sum: 34379217.4\n",
            "   Loss: 134856288.0\n",
            "   Took: 59.03849720954895 seconds\n",
            "Step 160\n",
            "   Sum: 34001406.0\n",
            "   Loss: 115254072.0\n",
            "   Took: 59.285794496536255 seconds\n",
            "Step 180\n",
            "   Sum: 33633042.2\n",
            "   Loss: 99298464.0\n",
            "   Took: 59.084718227386475 seconds\n",
            "Step 200\n",
            "   Sum: 33276685.1\n",
            "   Loss: 85694184.0\n",
            "   Took: 60.10506081581116 seconds\n",
            "Step 220\n",
            "   Sum: 32935662.8\n",
            "   Loss: 74128952.0\n",
            "   Took: 59.13915967941284 seconds\n",
            "Step 240\n",
            "   Sum: 32612310.0\n",
            "   Loss: 64302152.0\n",
            "   Took: 59.01610732078552 seconds\n",
            "Step 260\n",
            "   Sum: 32307050.2\n",
            "   Loss: 55867816.0\n",
            "   Took: 59.059996128082275 seconds\n",
            "Step 280\n",
            "   Sum: 32020719.9\n",
            "   Loss: 48570824.0\n",
            "   Took: 59.21238946914673 seconds\n",
            "Step 300\n",
            "   Sum: 31755843.6\n",
            "   Loss: 41868340.0\n",
            "   Took: 60.082600593566895 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XvXYhPTH7IrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "171f730e-58fc-41f2-8a20-667f0ddb2169"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print( os.getcwd() )\n",
        "print( os.listdir() )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "['sample_data', '.config', 'graphs', 'checkpoints', 'guernica.jpg', 'resized_deadpool.jpg', 'deadpool.jpg', 'guernica (1).jpg', 'imagenet-vgg-verydeep-19.mat', 'deadpool (1).jpg', 'outputs', 'resized_guernica.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kJ7-oXz-DU_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download( \"outputs/19.png\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQsDcjQtJmpf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}